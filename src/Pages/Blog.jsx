import React from "react";
import "../Styles/Hero.css";
import "../Styles/Blog.css";
import note from "../images/note.jpg";
import lab from "../images/lab.jpg";
import cameraMan from "../images/tech.jpeg";
import discussionImg from "../images/tech2.jpeg";

import menOnComputer from "../images/tech3.jpeg";
import sketch from "../images/sketch.png";
import table from "../images/table.png";
import chart from "../images/chart.png";

import { Link } from "react-router-dom";

const Blog = () => {
  return (
    <div>
      <section className="heroSection">
        <h1>Need our help ? Call us </h1>
        <p>
          The Tech Platform That Gives Back To The Community. Join the Lucky
          Winners
        </p>
        <Link>
          <button>LEARN MORE </button>
        </Link>
      </section>

      <div className="gridBlog">
        <section className="blogWrap">
          <div className="noteAndTextWrap">
            <div>
              <img src={note} alt="note-img" />
            </div>

            <div className="gridLinkAndText">
              <h5>UNCATEGORIZE</h5>

              <div>
                <Link>
                  Samsung Electronics Signs Net Zero Home Cooperation
                  partnership With Solaredge{" "}
                </Link>
              </div>
              <div>
                <small>june 16,2023 </small> <br /> <br />
              </div>
            </div>

            <div>
              <p>
                Samsung Electronics today announced that it has signed a
                partnership with SolarEdge, the global leader in residential
                solar and storage solutions, to expand its coverage of the Net
                Zero Home ecosystem.
              </p>
              <p>
                Integrating with SolarEdge Home, SolarEdge’s smart energy
                ecosystem, SmartThings Energy offers users new ways to optimize
                energy consumption and lower their home’s energy bills. It
                provides one-stop support, from monitoring a household’s
                photovoltaic (PV) generation and battery status at a glance to
                optimize energy consumption and save energy with its AI Energy
                Mode. The SolarEdge ecosystem has expanded to include Samsung’s
                wide range of smart energy technologies, including heat pumps to
                power more devices with clean solar energy, together providing
                visibility on appliances’ impact on household energy
                consumption.
              </p>

              <p>
                Samsung has now created a base for business expansion through
                this partnership with SolarEdge, a company specializing in smart
                inverters that convert sunlight into electrical energy.
                SolarEdge’s innovative energy optimization system uses
                data-driven automation to make hundreds of optimal energy
                decisions every day based on smart predictions of solar
                production, energy consumption patterns, battery status and
                utility rates.
              </p>

              <p>
                “The solutions we’ve developed with our key partners underline
                our commitment to creating a brighter future with solar power,”
                said Chan-woo Park, Executive Vice President of Digital
                Appliances Business at Samsung Electronics. “We’re excited to
                demonstrate how expanding our net zero home solution and
                SmartThings Energy service will make it easier for consumers to
                become energy independent, save money and contribute to creating
                a healthier planet.”
              </p>

              <p>
                “Samsung is a strategic partner, and we are excited to further
                expand the SolarEdge Home offering and provide our customers
                with an expanded suite of possible smart energy appliances,”
                said Ido Ginodi, VP of Global Product at SolarEdge. “As part of
                an end-to-end smart energy ecosystem, our smart SolarEdge ONE
                optimization system will help optimize home energy consumption
                and lower homeowner energy bills.”
              </p>
              <p>
                SmartThings Energy will also provide periodic information on
                carbon intensity as one of its new features, starting at the end
                of June. Carbon intensity refers to the carbon emissions
                generated by consuming one kilowatt-hour (kWh) of electricity,
                and consumers can experience the reduction in carbon emissions
                through product-specific carbon emission forecasts and
                participate by choosing relatively low carbon intensity time
                periods.
              </p>

              <p>
                In addition, Samsung will expand its Demand Response (DR)
                service globally starting in the U.S., following its successful
                introduction in Korea. Backed by local governments, the service
                provides financial incentives such as cash rewards and credits
                to consumers if they voluntarily reduce energy use during peak
                hours of power usage.
              </p>

              <p>
                Since March, Samsung has been operating DR in cooperation with
                the Seoul Metropolitan Government and the Korea Electric Power
                Corporation. Starting at the end of June, Samsung plans to test
                run DR in California and New York.
              </p>

              <p>
                Samsung’s collaboration with SolarEdge is not the first of its
                kind. It has a history of joining hands with a variety of solar
                energy companies, like Qcells in 2021, and also with SMA Solar
                Technology and Maxeon Solar Technology.
              </p>

              <p>
                Samsung will display its innovative, energy-saving solutions at
                Intersolar Europe 2023 together with its various global
                partners. The event, Europe’s largest energy exhibition, will
                take place in Munich, Germany from June 14 to 16.
              </p>
            </div>

            <div>
              <img src={note} alt="note-img" />
            </div>

            <div>
              <i>By Admin</i>
            </div>
          </div>

          <div className="noteAndTextWrap">
            <div>
              <img src={lab} alt="lab-img" />
            </div>

            <div className="gridLinkAndText">
              <h5>UNCATEGORIZE</h5>

              <div>
                <Link>
                  Samsung To ShowCase Latest Innovative C-Lab projects At
                  VivaTech 2023
                </Link>
              </div>
              <div>
                <small>june 16,2023 </small> <br /> <br />
              </div>
            </div>

            <div>
              <p>
                Samsung Electronics today announced it will be showcasing
                innovative projects developed through its C-Lab program at
                VivaTech 2023. VivaTech, Europe’s largest startup and tech
                event, will take place from June 14 to 17 at the Paris Expo
                Porte de Versailles exhibition and conference center.
              </p>
              <p>
                Samsung will set up a C-Lab exhibition space in the “K-Startup
                Hall”1 to display one project from C-Lab Inside, an internal
                venture program for employees, and four startups fostered by
                C-Lab Outside, a program for external startups.
              </p>

              <p>
                C-Lab projects and startups will be able to gauge the global
                market response and strengthen their business case in front of a
                large number of visitors at VivaTech. They can also take part in
                investment and business cooperation consultations on site.
              </p>

              <h3>
                Global Launch of Relumino Glass Brings Significant Improvements
                in Usability and Convenience
              </h3>

              <div>
                <img src={cameraMan} alt="manWIthCamera-img" />
              </div>
              <p>C-Lab Inside "Relumino"</p>
              <p>
                Relumino, a visual aid solution for people with low vision, was
                selected as a C-Lab Inside project in 2016 and has since been
                further developed and perfected by Samsung Research.
              </p>

              <p>
                The solution, whose name means “to give light back,” consists of
                the Relumino app, a smartphone image processing application that
                utilizes the residual vision of people with low vision to
                improve object recognition, and Relumino Glasses, a glasses-type
                wearable device.
              </p>

              <p>
                Having previously showcased its Relumino solution at MWC 2017
                and CES 2018, Samsung will introduce Relumino Glasses to the
                global market at VivaTech 2023 and demonstrate how it enhanced
                the technology’s user experience to provide greater comfort and
                prevent fatigue.
              </p>

              <p>
                “As the 2024 Paralympic Games will be held in Paris, France next
                year, there will be a lot of interest in apps and services for
                people with disabilities,” said Junghoon Cho, one of the Samsung
                researchers who developed Relumino. “This exhibition will be an
                opportunity for Relumino to take another step forward.”
              </p>

              <h3>
                Showcasing C-Lab Outside Startups in the Fields of AI, the
                Metaverse and More
              </h3>

              <p>
                Created in October 2018, C-Lab Outside is a startup acceleration
                program launched to invigorate the startup ecosystem in Korea.
                Startups enrolled in the C-Lab Outside program are provided with
                office workspaces, expert mentoring from Samsung employees,
                digital marketing and financial consulting. Additional support
                includes potential partnerships with Samsung and opportunities
                to participate in IT exhibitions such as CES, VivaTech and KES
                (Korea Electronics Show).
              </p>

              <p>
                Since the beginning of this year, Samsung has been accelerating
                the expansion of the domestic startup ecosystem by introducing a
                series of C-Lab Outside programs. Through these efforts, the
                program is expected to revitalize the local economy, create
                high-quality jobs and further contribute to the balanced
                development of Korea.
              </p>

              <p>
                At VivaTech 2023, Samsung will support the exhibition of
                selected companies not only from C-Lab Outside Seoul, but also
                Daegu and Gwangju, giving local startups a chance to expand into
                the global market.
              </p>

              <p>
                “Thanks to Samsung Electronics’ support for VivaTech, we are one
                step closer to entering Europe,” said Nayul Kim, CEO of CLIKA, a
                startup that was selected for C-Lab Outside Gwangju. “We will
                continue to promote the excellence of Korean startups.”
              </p>

              <p>
                The companies participating in VivaTech were selected from a
                pool of C-Lab Outside startups that are aiming to enter the
                European market. The four chosen startups include:
              </p>

              <div>
                <ul>
                  <li>NdotLight — creator of a web-based 3D design solution</li>
                  <li>
                    Vsion — maker of super clear PDLC film and reverse mode PDLC
                    film
                  </li>
                  <li>
                    QuantumCat — the first company to commercialize gold
                    nanocatalyst technology
                  </li>

                  <li>
                    CLIKA — provider of an all-in-one Auto-TinyAI platform that
                    helps companies commercialize AI models quickly and reliably
                  </li>
                </ul>
              </div>

              <h3>
                Fostering 866 Startups and Projects Over the Last 10 Years
              </h3>

              <p>
                Samsung operates C-Lab to create sustainable innovations and
                contribute to the revitalization of Korea’s domestic startup
                ecosystem.
              </p>

              <p>
                Launched in December 2012, C-Lab Inside nurtures employees’
                innovative ideas while instilling a corporate culture that puts
                creativity at the fore. The program supports the development of
                ideas from all areas of business. Leveraging the success of the
                C-Lab Inside initiative, C-Lab Outside has been expanding
                Samsung’s support for new ventures to startups and innovations
                outside of the Samsung network since 2018.
              </p>

              <p>
                The company has nurtured a total of 866 startups and projects to
                date, including 475 through C-Lab Outside and 391 through C-Lab
                Inside
              </p>
            </div>

            <div>
              <img src={discussionImg} alt="discussionImg-img" />
              <p>c-Lab Outside "CLIKA</p>
            </div>
            <div>
              <img src={menOnComputer} alt="menOnComputer-img" />
            </div>
            <div>
              <i>By Admin</i>
            </div>
          </div>

          <div className="noteAndTextWrap">
            <div>
              <img src={sketch} alt="sketch-img" />
            </div>
            <div className="gridLinkAndText">
              <h5>UNCATEGORIZE</h5>

              <div>
                <Link>
                  {"[CVPR Series #6]"} LASP: Text-To-Text Optimization For
                  Language-Aware Soft Prompting Of Visions & Language Models
                </Link>
              </div>
              <div>
                <small>june 16,2023 </small> <br /> <br />
              </div>
            </div>
            <div>
              <p>
                The Computer Vision and Pattern Recognition Conference (CVPR) is
                a world-renowned international Artificial Intelligence (AI)
                conference co-hosted by the Institute of Electrical and
                Electronics Engineers (IEEE) and the Computer Vision Foundation
                (CVF) which has been running since 1983. CVPR is widely
                considered to be one of the three most significant international
                conferences in the field of computer vision, alongside the
                International Conference on Computer Vision (ICCV) and the
                European Conference on Computer Vision (ECCV).In this relay
                series, we are introducing a summary of the 7 research papers at
                the CVPR 2023 and here is a summary of them.– Part 1 :
                SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with
                Neural Radiance Fields (by Samsung AI Center – Toronto)- Part 2
                : StepFormer: Self-supervised Step Discovery and Localization in
                Instructional Videos (by Samsung AI Center – Toronto)– Part 3 :
                GENIE: Show Me the Data for Quantization (by Samsung Research)–
                Part 4 : A Unified Pyramid Recurrent Network for Video Frame
                Interpolation (By Samsung R&D Institute – Nanjing)– Part 5 :
                MobileVOS: Real-Time Video Object Segmentation Contrastive
                Learning Meets Knowledge Distillation (By Samsung R&D Institute
                United Kingdom)– Part 6 : LASP: Text-to-Text Optimization for
                Language-Aware Soft Prompting of Vision & Language Models (By
                Samsung AI Center – Cambridge)
              </p>

              <h3>Introduction</h3>
              <p>
                Welcome to our research blog post, where we delve into the
                fascinating world of vision and language models. In recent
                years, large-scale pre-training of neural networks has paved the
                way for ground-breaking advancements in Vision & Language (V&L)
                understanding. These pre-trained models, such as BERT [5] and
                CLIP [1], have demonstrated their ability to capture the
                intricacies of the world, enabling them to adapt seamlessly to
                new tasks and datasets.
              </p>

              <p>
                In this blog post, we focus on the remarkable potential of V&L
                models trained with contrastive learning, which have opened
                doors to few-shot and even zero-shot adaptation. By leveraging
                the power of contrastive learning, these models can quickly
                adapt to new downstream tasks with minimal training examples.
                Prompt engineering and learning have emerged as powerful
                techniques for adapting V&L models to novel tasks, drawing
                inspiration from their counterparts in Natural Language
                Processing (NLP). Initially, researchers relied on manual
                templates or prompts to create class-specific weights for
                zero-shot recognition. However, recent advancements have
                introduced the concept of “soft prompts” [2,3] – learnable
                vectors that are fed into the text encoder along with the class
                name. These soft prompts are learned from a few training
                examples with the entire V&L model kept frozen. The whole
                process can be seen as parameter efficient fine-tuning of the
                model on a small training dataset.
              </p>

              <p>
                Despite the promising results of soft prompt learning, there is
                a noticeable challenge known as base class overfitting. While
                accuracy on the base classes improves significantly, the
                accuracy on unseen novel classes suffers. This issue arises
                because soft prompts are learned from only a few examples
                belonging to the base classes. Interestingly, hand-engineered
                prompts still outperform existing soft prompt learning methods
                when it comes to recognizing novel classes.
              </p>

              <h3>Our Approach</h3>
              <p>
                To address the problem of base class overfitting, we propose a
                simple, yet highly effective solution motivated by a keen
                observation: Prompt learning enhances accuracy on base classes,
                while prompt engineering excels in recognizing novel classes.
                Drawing on this insight, we introduce a novel text-to-text loss
                that enforces the learned prompts to be close, in embedding
                space, to the textual prompts. By exploiting the intrinsic
                information captured by the text encoder, we enable
                language-only optimization for V&L model adaptation, a unique
                approach compared to previous soft prompt learning methods that
                mainly focus on V&L interactions.
              </p>
              <h3>Our Contributions</h3>
              <p>
                We propose a novel framework for soft prompt learning which we
                call Language-Aware Soft Prompting (LASP). Our main
                contributions within the LASP framework are as follows:
              </p>

              <ul className="ulNumber">
                <li>
                  Grouped Language-Aware Prompt Representation:To increase the
                  representation capacity of prompts, we draw inspiration from
                  grouped convolution and multi-head attention. We introduce a
                  grouped language-aware prompt representation, where each group
                  of prompts specializes in a different subset of pre-defined
                  manual templates.
                </li>

                <li>
                  Addressing Visual-Language Misalignment: Prompt learning and
                  more generally, contrastive pre-training, introduce a
                  visual-language misalignment that impacts generalization. To
                  tackle this challenge, we propose a re-calibration mechanism,
                  which involves Layer Normalization fine-tuning and learning a
                  class-agnostic bias.
                </li>

                <li>
                  Training with Virtual Classes:Leveraging our language-only
                  learning framework, we propose training LASP with virtual
                  classes, even when visual samples are unavailable. This
                  strategy further enhances the robustness of the learned
                  prompts.
                </li>
              </ul>

              <p>
                Through extensive experiments, we showcase the superiority of
                our approach over existing soft prompting methods. Our methods
                set a new state-of-the-art for few-shot and zero-shot image
                classification on multiple datasets. Notably, we present a
                prompt learning method that outperforms the strong baseline of
                hand-crafted prompts and CLIP for recognizing novel classes.
              </p>
            </div>
            <div>
              <img src={sketch} alt="sketch-img" />
            </div>
            <h3>
              Language-Only Optimisation: Language-Aware Soft Prompting (LASP)
            </h3>
            <p>
              Our method, called Language-Aware Soft Prompting (LASP) aims to
              enhance the generalisation and robustness of few-shot adaptation
              in Vision & Language (V&L) models. Unlike previous methods that
              primarily focus on V&L interactions, LASP leverages language-only
              optimization to improve generalization and mitigate base-class
              overfitting.
            </p>
            <p>
              As such, in addition to the vision-language classification loss,
              we introduce a second cross-entropy loss function that minimizes
              the distance between learned soft prompts and hand-engineered
              textual prompts. The textual prompts, obtained by encoding
              class-specific templates, act as class weights in the language
              space. This loss encourages the learnable prompts to be correctly
              classified based on the textual prompts. This support hard prompts
              can be randomly constructed or formed simply by taking the ones
              used in [1]. During training, we then calculate the probability of
              a learnable prompt being classified as a specific class. This
              probability calculation is based on measuring the cosine
              similarity between the encoded textual prompt for the target class
              and the encoded learnable prompt in the embedding space of CLIP’s
              text encoder.
            </p>
            <p>
              he overall training objective is then a combination of the V&L
              loss and the language-aware loss, weighted by user-defined scaling
              coefficients. This combined objective ensures that the model
              learns both visual and language cues effectively, leading to
              improved performance on novel classes and reduced overfitting to
              base classes. The overall framework is depicted in Figure 1.
            </p>
            <p>
              Intuitively, our method can be interpreted in several ways: as a
              regularizer, as a language-based augmentation and as a data-free
              distillation.
            </p>
            <p>
              LASP as a regularizer: As the proposed loss encourages the learned
              prompts to be close in the embedding space to the textual ones,
              our method can be viewed a s a regularizer that prevents the
              leaned prompt-conditioned features form diverging too much from
              the hand-crafted ones.
            </p>
            <p>
              LASP as a language-based augmentation:The current best practice
              for learning from visual data is to randomly apply a set of
              transformations (such as rotation, scaling etc.) at train time.
              Our text-to-text optimisation opens the door for a language-based
              augmentation for V&L adaptation too. In practice, we can achieve
              this by tareted prompting, where we can specify certain
              characteristics and/or apply text-based transformations to the
              class name, e.g.: “A sketch of dog” or “A rotated photo of a dog”.
            </p>
            <p>
              LASP as a data-free distillation:Typically, knowledge distillation
              requires a training set of images, where a teacher network
              provides a training signal for the student. LASP’s text-to-text
              loss can be also interpreted as a data-free distillation (i.e.
              does not use any image data) where the learnable prompts define
              the “samples”. As CLIP learns a joint V&L space, similar concepts
              are close together across both domains. Hence, optimizing against
              a concept or object in the language domain, using the proposed
              loss, should also help make a step in the visual domain, improving
              the classification of the images. Furthermore, LASP leverages the
              joint V&L space of CLIP to improve image classification even
              without using any image data. By optimizing against the textual
              prompts, LASP effectively distils knowledge from the language
              domain to enhance the model’s performance in the visual domain.
            </p>
            <h3>Grouped Language-Aware Prompt Representation</h3>
            <p>
              Building on the success of techniques like grouped convolutions
              and multi-head attention, we propose a new approach called Grouped
              Prompt Representation. This approach aims to optimize prompt
              learning by dividing the set of textual prompts into separate
              groups, where each group specializes in a specific subset of
              prompts: similarly, to how grouped convolutions and multi-head
              attention combine the expertise of individual groups or heads, our
              grouped prompt representation leverages the specialization of each
              group to enhance prompt learning
            </p>
            <p>
              To create the groups, we evenly split the set of prompts into
              multiple subsets. Each subset is associated with a specific group
              and is optimized to capture unique aspects of the prompts. These
              specialized prompt groups learn transformations tailored to their
              respective subset of prompts. The model is then trained using an
              adapted text-to-text loss that extends the original one to
              incorporate the grouped prompts. This loss ensures that the model
              accurately predicts the class probabilities based on the prompts
              from each group.
            </p>
            <p>
              During inference, the final prediction is obtained by averaging
              the similarity scores between each group’s text feature and the
              visual feature. This aggregation strategy combines the information
              from different groups to make a robust and comprehensive
              prediction.
            </p>
            <h3>Addressing Re-aligning LASP</h3>
            <p>
              For some downstream tasks, there might be a discrepancy between
              the data distribution of the downstream image dataset and the one
              used during CLIP training. It is crucial to address this data
              distribution shift in the downstream adaptation process. However,
              optimizing the visual encoder directly to account for this shift
              can lead to overfitting on the base classes, where the V&L
              embeddings are pushed away from the joint space learned by CLIP.
              Early experiments with visual adapters have shown a negative
              impact on zero-shot accuracy.
            </p>
            <p>
              To overcome this challenge, we propose an alternative approach:
              fine-tuning the Layer Normalization (LN) of the CLIP encoder.
              Fine-tuning the LN parameters provides a more robust way to adapt
              the visual encoder while maintaining alignment with the joint
              space learned by CLIP. By selectively adjusting the LN parameters,
              the model can capture the distributional shift without sacrificing
              zero-shot accuracy. This fine-tuning process helps the model
              effectively combat data distribution shift during downstream
              adaptation.
            </p>
            <p>
              After fine-tuning the LN parameters, there is a possibility of
              misalignment between the visual and language modalities. To
              address this issue, we propose learning a “correction” at the
              output of the text encoder in the form of a learnable offset or
              bias. This offset aims to re-align the two modalities and ensure
              their compatibility. Specifically, we introduce a learnable vector
              b, which is summed to the set of weights W of the linear
              classifier obtained by passing the learned prompts to the text
              encoder. It is important to note that the learned offset is shared
              among all classes, allowing it to be readily applied even in the
              case of novel classes. This approach enables us to effectively
              correct for V&L misalignment and improve the overall alignment and
              compatibility between the two modalities.
            </p>
            <h3>Training with Virtual Classes (LASP-V)</h3>
            <p>
              A direct observation that can be drawn from the text-to-text
              optimisation loss is that, in practice, we do not have to use only
              the class names for which we have labelled image data, as the
              value of LASP is independent of the input image. To this end, we
              propose to learn the prompts using both annotated image-text pairs
              and class names outside the base set (for which we have no images
              available). We call this setting as training LASP with virtual
              classes. Our setting combines the best of both words: the guidance
              from the few annotated image samples and the zero-shot
              generalizability of language-based training. As we will show
              bellow, LASP with virtual classes can significantly improve the
              robustness of the prompts learned. We refer to this variant of our
              method in the tables bellow as LASP-V.
            </p>
            <h3>Experimental Results</h3>
            <p>
              Our method is trained and tested across a suite formed of 12
              datasets (i.e., ImageNet, Caltech-101, Oxford Pets, Stanford Cars,
              Flowers-102, Food-101, FGVC Aircraft, SUN397, DTD, EuroSAT and
              UCF-101). Each dataset is split into a base and novel set, each
              containing samples belonging to an equal number of disjoint
              classes. The training is then performed in a few-shot manner
              (herein, 16 samples per class) on the base set while the testing
              on both base and new, separately. As the results from Table 1
              show, our method outperforms prior work by a large margin,
              especially for the new set, where notably, our approach
              outperforms CLIP with hand crafted prompts, with LASP-V (with
              virtually classes) setting a new state-of-the-art.
            </p>
            <div>
              <img src={table} alt="table-img" />
            </div>
            <p>
              Table 1.Aggregated classification accuracy (top-1) across a suite
              of 12 datasets for the base and new set. H represents the harmonic
              mean of the two.
            </p>
            <p>
              While the improvements vary depending on the distribution of the
              data, on certain datasets, such as EuroSAT the gains are as large
              as 17% (see Figure 2).
            </p>
            <div>
              <img src={chart} alt="chart-img" />
            </div>
            <p>
              Figure 2. Classification accuracy (top-1) on the novel classes
              subset on EuroSAT, DTD, UCF-101 and SUN397 datasets.
            </p>
            <h3>Conclusion</h3>
            <p>
              In summary, in this work, we introduced LASP – a language aware
              soft prompting method for V&L adaptation that is shown to
              outperform prior work by large margin. Our work is the first to
              propose a text-to-text optimisation framework for vision language
              adaption, largely alleviating the problem of base-class
              overfitting. Moreover, we showed that our approach, unlike prior
              work, is amenable to, including during training, virtual classes,
              i.e., class names for which no visual samples are available,
              significantly increasing the robustness of the learned prompts. We
              hope that LASP/LASP-V will serve as a strong baseline and
              steppingstone for future works in the area of few-shot adaptation
              for V&L models.
            </p>
            <h3>Link to the paper</h3>
            <small className="small">
              https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf
            </small>
            <h3>References</h3>
            <p>
              {" "}
              {"[1]"} Learning transferable visual models from natural language
              supervision, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
              Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
              Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya
              Sutskever, ICML 2021
            </p>
            <p>
              {"[2]"} Learning to Prompt for Vision-Language Models, Kaiyang
              Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, IJCV 2022
            </p>
            <p>
              {"[3]"} Conditional Prompt Learning for Vision-Language Models,
              Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, CVPR 2022
            </p>
            <p>
              {"[4]"} Prompt Distribution Learning, Yuning Lu, Jianzhuang Liu,
              Yonggang Zhang, Yajing Liu, Xinmei Tian, CVPR 2022
            </p>

            <p>
              {"[5]"} BERT: Pre-training of Deep Bidirectional Transformers for
              Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee,
              Kristina Toutanova, NAACL 2
            </p>
            <div>
              <i>By Admin</i>
            </div>
          </div>
        </section>

        <section className="blogsNewsWrap">
          <div className="padBox">
            <div className="flexInputAndBut">
              <input type="text" />
              <button>search</button>
            </div>
          </div>

          <div className="padBox">
            <hr />
          </div>

          <div className="padBox">
            <h3>Recent Posts</h3>
            <div>
              <Link>
                {" "}
                Samsung Electronics Signs Net Zero Home Cooperation partnership
                With Solaredge{" "}
              </Link>
            </div>{" "}
            <br /> <br />
            <div>
              <Link>
                {" "}
                Samsung To ShowCase Latest Innovative C-Lab projects At VivaTech
                2023
              </Link>
            </div>
            <br /> <br />
            <div>
              <Link>
                {" "}
                {"[CVPR Series #6]"} LASP: Text-To-Text Optimization For
                Language-Aware Soft Prompting Of Visions & Language Models
              </Link>
            </div>
            <div>
              <h3>Recent Comments</h3>

              <p>No comments to show </p>

              <div>
                <hr />
              </div>
            </div>
          </div>

          <div className="padBox">
            <h3>Archives </h3>
          </div>

          <ul>
            <li>june 2023 </li>
          </ul>

          <div className="padB">
            <h3>Categories</h3>
          </div>
          <ul>
            <li>uncategorized </li>
          </ul>

          <div className="padBox">
            <h3>About Exponent </h3>
            <p>
              Exponent is a modern business theme, that lets you build stunning
              high performance websites using a fully visual interface. Start
              with any of the demos below or build one on your own.
            </p>

            <button className="get">GET START </button>
          </div>

          <div className="flexImgAndTextLast">
            <div>
              <img src={note} alt="note-img" />
            </div>

            <div>
              <p>Samsung Electronics Signs Net Zero Home Cooperation</p>
              <small>
                <b>Jun 16, 2023</b>
              </small>
            </div>
          </div>

          <div className="flexImgAndTextLast">
            <div>
              <img src={lab} alt="lab-img" />
            </div>

            <div>
              <p>Samsung To Showcase Latest Innovative C-Lab Project</p>
              <small>
                <b>Jun 16, 2023</b>
              </small>
            </div>
          </div>
          

            <div className="flexImgAndTextLast">
            <div>
              <img src={sketch} alt="sketch-img" />
            </div>

            <div>
              <p>[CVPR 2023 Series #6] LASP: Text-to-Text Optimization</p>
              <small>
                <b>Jun 16, 2023</b>
              </small>
            </div>
          </div>
        </section>
      </div>
    </div>
  );
};

export default Blog;
